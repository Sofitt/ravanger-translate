#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∞–π–ª–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ LLM (–≤–µ—Ä—Å–∏—è 2)
–° –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞—Ö
–†–∞–±–æ—Ç–∞–µ—Ç —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ .rpy —Ñ–∞–π–ª–∞–º–∏ –∏–∑ extracted_scripts/
"""

import os
import re
import json
import argparse
from datetime import datetime
from typing import List, Dict, Tuple, Set, Optional
from collections import defaultdict


class TranslationPreparerV2:
    # –°–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞ RenPy, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏
    RENPY_KEYWORDS = {
        'if', 'else', 'elif', 'while', 'for', 'pass', 'return', 'jump', 'call',
        'menu', 'centered', 'extend', 'nvl', 'scene', 'show', 'hide', 'with',
        'window', 'play', 'stop', 'pause', 'define', 'default', 'label', 'init',
        'python', 'transform', 'image', 'screen', 'style', 'translate', 'narrator',
        'pov', 'variant', 'textbutton', 'text_selected_color', 'text_hover_color',
        'text_font', 'text_color', 'text', 'style_suffix', 'layout',
        'mousewheel', 'scrollbars', 'size_group', 'style_group', 'style_prefix',
        'add', 'alt', 'auto','background', 'draggable', 'font', 'foreground',
        'hover_background', 'id', 'idle', 'key',
    }

    def __init__(self):
        self.strings = []
        self.character_entities = set()  # –í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π

    def parse_rpy_source(self, file_path: str) -> Tuple[List[Dict], Set[str]]:
        """–ü–∞—Ä—Å–∏—Ç –∏—Å—Ö–æ–¥–Ω—ã–π .rpy —Ñ–∞–π–ª –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–µ–≤–æ–¥–∏–º—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π"""

        if not os.path.exists(file_path):
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")

        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        strings = []
        character_entities = set()
        current_speaker = None  # –¢–µ–∫—É—â–∏–π –≥–æ–≤–æ—Ä—è—â–∏–π (–∏–∑ show –∏–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Ä–µ–ø–ª–∏–∫–∏)

        idx = 0
        for line_num, line in enumerate(lines, 1):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            stripped = line.strip()
            if not stripped or stripped.startswith('#'):
                continue

            # –ö–æ–º–∞–Ω–¥–∞ show - –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—É—â–µ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
            show_match = re.match(r'\s+show\s+(\w+)', line)
            if show_match:
                current_speaker = show_match.group(1)
                continue

            # –°–∫—Ä—ã–≤–∞–µ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
            hide_match = re.match(r'\s+hide\s+(\w+)', line)
            if hide_match:
                if current_speaker == hide_match.group(1):
                    current_speaker = None
                continue

            # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤ —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
            # –§–æ—Ä–º–∞—Ç: –ø–µ—Ä—Å–æ–Ω–∞–∂ [talk] "—Ç–µ–∫—Å—Ç"
            dialog_match = re.match(r'\s+(\w+)\s+(?:talk\s+)?"([^"]*)"', line)
            if dialog_match:
                speaker = dialog_match.group(1)
                text = dialog_match.group(2)

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞ RenPy
                if speaker in self.RENPY_KEYWORDS:
                    continue

                character_entities.add(speaker)

                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
                analysis = self._analyze_text(text)

                strings.append({
                    "id": idx,
                    "line": line_num,
                    "file": os.path.basename(file_path),
                    "speaker": speaker,
                    "speaker_prefix": self._get_speaker_prefix(speaker),
                    "original": text,
                    "translation": "",
                    "context": self._detect_context(text, speaker),
                    "tags": analysis["tags"],
                    "variables_curly": analysis["variables_curly"],
                    "variables_square": analysis["variables_square"],
                    "special_chars": analysis["special_chars"]
                })

                idx += 1
                current_speaker = speaker
                continue

            # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∞–Ω–æ–Ω–∏–º–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ (–ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ)
            # –§–æ—Ä–º–∞—Ç: "—Ç–µ–∫—Å—Ç"
            anon_match = re.match(r'\s+"([^"]*)"', line)
            if anon_match:
                text = anon_match.group(1)

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                if not text.strip() or len(text.strip()) < 3:
                    continue

                analysis = self._analyze_text(text)

                strings.append({
                    "id": idx,
                    "line": line_num,
                    "file": os.path.basename(file_path),
                    "speaker": current_speaker if current_speaker else "narrator",
                    "speaker_prefix": self._get_speaker_prefix(current_speaker) if current_speaker else "narrator",
                    "original": text,
                    "translation": "",
                    "context": self._detect_context(text, current_speaker),
                    "tags": analysis["tags"],
                    "variables_curly": analysis["variables_curly"],
                    "variables_square": analysis["variables_square"],
                    "special_chars": analysis["special_chars"]
                })

                idx += 1
                continue

        return strings, character_entities

    def _get_speaker_prefix(self, speaker: Optional[str]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ (n = narrator)"""
        if not speaker:
            return "narrator"

        if speaker.startswith('n') and len(speaker) > 1:
            return 'n'  # nprincess, nmaid –∏ —Ç.–¥.

        return speaker

    def _analyze_text(self, text: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–≥–∏, –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã"""

        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ —Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö
        variables_curly = re.findall(r'\{(\w+)\}', text)

        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö (–≤–∫–ª—é—á–∞—è —Å–ª–æ–∂–Ω—ã–µ —Ç–∏–ø–∞ [namepov!tc])
        variables_square = re.findall(r'\[([^\]]+)\]', text)

        # –¢–µ–≥–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        tags = re.findall(r'\{/?(?:color|b|i|u|size|center|w|nw|p|fast)[^}]*\}', text)

        # –°–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
        special_chars = {
            "newlines": text.count('\\n'),
            "tabs": text.count('\\t'),
            "escaped_quotes": text.count('\\"'),
            "has_formatting": bool(tags)
        }

        return {
            "variables_curly": variables_curly,
            "variables_square": variables_square,
            "tags": tags,
            "special_chars": special_chars
        }

    def _detect_context(self, text: str, speaker: Optional[str]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç—Ä–æ–∫–∏"""

        # –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –æ–±—ã—á–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–µ
        if len(text) < 20 and not any(c in text for c in '.!?'):
            return "ui"

        # –ü–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ (narrator prefix)
        if speaker and speaker.startswith('n') and len(speaker) > 1:
            return "narration"

        # –î–∏–∞–ª–æ–≥–∏ –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        if any(c in text for c in '.!?'):
            return "dialogue"

        return "text"

    def save_character_map(self, character_entities: Set[str], output_file: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç map –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ JSON —Ñ–∞–π–ª –¥–ª—è –ø—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ–ª–∞"""

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞—Ç—É –∫ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        if output_file:
            dir_name = os.path.dirname(output_file)
            base_name = os.path.basename(output_file)
            name, ext = os.path.splitext(base_name)

            # –§–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: –¥–µ–Ω—å.–º–µ—Å—è—Ü.–≥–æ–¥
            date_str = datetime.now().strftime("%d.%m.%Y-%H:%M:%S")
            output_file = os.path.join(dir_name, f"{name}_{date_str}{ext}")

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –±–∞–∑–æ–≤–æ–º—É –∏–º–µ–Ω–∏ (–±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ n)
        character_map = {}

        for entity in sorted(character_entities):
            base_name = entity.lstrip('n') if entity.startswith('n') and len(entity) > 1 else entity

            if base_name not in character_map:
                character_map[base_name] = {
                    "entities": [],
                    "gender": "",  # –ó–∞–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤—Ä—É—á–Ω—É—é: "male", "female", "neutral", "unknown"
                    "notes": ""
                }

            character_map[base_name]["entities"].append(entity)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(character_map, f, ensure_ascii=False, indent=2)

        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞—Ä—Ç–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π: {output_file}")
        print(f"   –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π: {len(character_map)}")
        print(f"   –í—Å–µ–≥–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {len(character_entities)}")

    def load_character_map(self, character_map_file: str) -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç map –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å entity -> gender"""

        if not os.path.exists(character_map_file):
            return {}

        with open(character_map_file, 'r', encoding='utf-8') as f:
            character_map = json.load(f)

        # –°–æ–∑–¥–∞–µ–º –ø–ª–æ—Å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å entity -> gender
        entity_gender = {}
        for base_name, info in character_map.items():
            gender = info.get("gender", "")
            for entity in info["entities"]:
                entity_gender[entity] = gender

        return entity_gender

    def enrich_with_gender(self, strings: List[Dict], entity_gender: Dict[str, str]):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–ª–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –≤ —Å—Ç—Ä–æ–∫–∏"""

        for string in strings:
            speaker = string.get("speaker", "")
            string["speaker_gender"] = entity_gender.get(speaker, "unknown")

    def prepare_module(self, module_file: str, output_file: str,
                      character_map_file: Optional[str] = None,
                      save_char_map: bool = True):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –º–æ–¥—É–ª—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞"""

        print(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {module_file}")

        strings, character_entities = self.parse_rpy_source(module_file)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞—Ä—Ç—É –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        if save_char_map:
            char_map_output = character_map_file or output_file.replace('.json', '_characters.json')
            self.save_character_map(character_entities, char_map_output)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–ª–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        entity_gender = {}
        if character_map_file and os.path.exists(character_map_file):
            entity_gender = self.load_character_map(character_map_file)
            self.enrich_with_gender(strings, entity_gender)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–ª–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π")

        # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total = len(strings)

        metadata = {
            "source_file": os.path.basename(module_file),
            "source_path": module_file,
            "total_strings": total,
            "character_count": len(character_entities),
            "characters": list(sorted(character_entities)),
            "source_language": "en",
            "target_language": "ru"
        }

        output = {
            "metadata": metadata,
            "strings": strings
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ: {total} —Å—Ç—Ä–æ–∫")
        print(f"   –ü–µ—Ä—Å–æ–Ω–∞–∂–µ–π: {len(character_entities)}")
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {output_file}")

        return output

    def prepare_batch(self, source_dir: str, output_dir: str,
                     pattern: str = "*.rpy",
                     character_map_file: Optional[str] = None):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞–∫–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""

        import glob

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        source_files = glob.glob(os.path.join(source_dir, pattern))

        print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(source_files)}")

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        all_characters = set()

        for source_file in source_files:
            basename = os.path.basename(source_file).replace('.rpy', '_v2.json')
            output_file = os.path.join(output_dir, basename)

            try:
                result = self.prepare_module(
                    source_file,
                    output_file,
                    character_map_file=character_map_file,
                    save_char_map=False  # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
                )
                all_characters.update(result["metadata"]["characters"])
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {source_file}: {e}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â—É—é –∫–∞—Ä—Ç—É –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
        if character_map_file:
            self.save_character_map(all_characters, character_map_file)

        print(f"\nüéâ –ì–æ—Ç–æ–≤–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(source_files)}")
        print(f"   –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π: {len(all_characters)}")


def main():
    parser = argparse.ArgumentParser(
        description="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –¥–ª—è LLM (v2 —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

  # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
  python llm_translate_prepare_v2.py --source ../extracted_scripts/c1.rpy --output ../temp_files/c1_v2.json

  # –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å —Å–æ–∑–¥–∞–Ω–∏–µ–º –∫–∞—Ä—Ç—ã –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
  python llm_translate_prepare_v2.py --batch ../extracted_scripts --batch-output ../temp_files/llm_json_v2 --character-map ../data/characters.json

  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–∞—Ä—Ç—ã –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
  python llm_translate_prepare_v2.py --source ../extracted_scripts/c1.rpy --output ../temp_files/c1_v2.json --character-map ../data/characters.json

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –§–∞–π–ª –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Å –¥–∞—Ç–æ–π, –Ω–∞–ø—Ä–∏–º–µ—Ä: characters_11.10.2025-23:59:59.json
        """
    )

    parser.add_argument("--source", help="–ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É .rpy —Ñ–∞–π–ª—É")
    parser.add_argument("--output", help="–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É JSON —Ñ–∞–π–ª—É")
    parser.add_argument("--batch", help="–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
    parser.add_argument("--batch-output", default="../temp_files/llm_json_v2",
                       help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö JSON (–ø—Ä–∏ --batch)")
    parser.add_argument("--pattern", default="*.rpy",
                       help="–ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ (–ø—Ä–∏ --batch)")
    parser.add_argument("--character-map",
                       help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∫–∞—Ä—Ç–æ–π –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π (–¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)")

    args = parser.parse_args()

    preparer = TranslationPreparerV2()

    if args.batch:
        preparer.prepare_batch(
            args.batch,
            args.batch_output,
            args.pattern,
            character_map_file=args.character_map
        )
    elif args.source and args.output:
        preparer.prepare_module(
            args.source,
            args.output,
            character_map_file=args.character_map
        )
    else:
        # –†–µ–∂–∏–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ extracted_scripts
        source_dir = "../extracted_scripts"
        output_dir = "../temp_files/llm_json_v2"
        character_map = "../data/characters.json"

        print("üöÄ –†–µ–∂–∏–º –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        print(f"üìÅ –í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {source_dir}")
        print(f"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
        print(f"üìÅ –ö–∞—Ä—Ç–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π: {character_map}")
        print()

        preparer.prepare_batch(source_dir, output_dir, character_map_file=character_map)


if __name__ == "__main__":
    main()
